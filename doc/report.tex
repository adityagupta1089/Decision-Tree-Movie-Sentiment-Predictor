\documentclass{article}
\usepackage{tikz}
\usepackage{amsmath}
\usetikzlibrary{datavisualization}
\title{CSL 603 - Machine Learning\\Lab 1\\Decision Trees and Forests}
\author{Aditya Gupta\\2015csb1003}
\begin{document}
\maketitle
\section*{Decision Tree (Experiment 2)}
A decision tree was learnt using 1000 reviews from the train set and tested on 1000 reviews from the test set. The learned tree showed the following statistics:
\subsection{Effect of Early Stopping}
The tree propagation was stopped when a critical height was reached and various properties of the tree such as total terminal nodes (leaf nodes) and the accuracy was noted. The following results were obtained:
\begin{itemize} 
\item Increase in Training Accuracy as maximum height increased as tree more and more fitted to the training data. The Testing accuracy initially increased but then became almost constant and then slightly decreased due to overfitting.

\begin{tikzpicture}[scale=1.5]
\datavisualization [scientific axes=clean,
	x axis={grid, label={Maximum Height of the Tree}},
	y axis={grid, label={Accuracy}, ticks ={tick unit=\%}},
	visualize as smooth line/.list={trainacc, testacc},
	style sheet=vary hue,
	style sheet=cross marks,
	trainacc={label in legend={text=Accuracy on Training Data}},
	testacc={label in legend={text=Accuracy on Testing Data}}]
data [set=trainacc] {
	x, y
	1, 58.148
	2,65.593
	3,69.818
	4,70.523
	5,72.233
	6,75.251
	7,76.257
	8,79.476
	9,82.494
	10,85.513
	20,96.277
	50,100.000
}
data [set=testacc] {
	x, y
	1,56.142
	2,65.888
	3,66.395
	4,66.598
	5,67.309
	6,66.802
	7,66.903
	8,64.974
	9,67.411
	10,64.771
	20,65.076
	50,63.959
};
\end{tikzpicture}

\item The total number of leaf nodes increased slowly as height increased as more and more splits occured and thus resulting in more leaf nodes.

\begin{tikzpicture}[scale=1.5]
\datavisualization [scientific axes=clean,
	visualize as smooth line/.list={leaf},
	style sheet=vary hue,
	style sheet=cross marks,
	x axis={grid, label={Maximum height of the Tree}},
	y axis={grid, label={Total Leaf Nodes}},
	leaf={label in legend={text=Total Leaf Nodes}}]
data [set=leaf]{
x, y
1,2
2,4
3,7
4,11
5,15
6, 23
7, 31
8, 39
9, 50
10, 60
20, 105
50, 127
};
\end{tikzpicture}

\item Some of the commonly used words to decide were ``this'', ``where'', ``town'', ``moments'' and ``he''.

\end{itemize}

\section*{Noise (Experiment 3)}
The Noise was changed and the following results were obtained:

\begin{tikzpicture}[scale=1.5]
\datavisualization [scientific axes=clean,
	x axis={grid, label={Noise Threshold}, ticks={tick unit=\%}},
	y axis={grid, label={Accuracy}, ticks ={tick unit=\%}},
	visualize as smooth line/.list={trainacc, testacc},
	style sheet=vary hue,
	style sheet=cross marks,
	trainacc={label in legend={text=Accuracy on Training Data}},
	testacc={label in legend={text=Accuracy on Testing Data}}]
data [set=trainacc] {
	x, y
	0, 100.0
	0.5, 100.0
	1, 100.0
	5, 100.0
	10, 100.0
}
data [set=testacc] {
	x, y
	0, 66.232
	0.5, 66.332
	1, 65.430
	5, 66.533
	10, 61.422
};
\end{tikzpicture}

\section*{Pruning (Experiment 4)}
Each rule was pruned by removing antedecents greedily increasing the rule's accuracy and finally the sorted rules were used to calculate the accuracy of the rules.

\begin{tikzpicture}[scale=1.5]
\datavisualization [scientific axes=clean,
	visualize as smooth line/.list={leaf},
	style sheet=vary hue,
	style sheet=cross marks,
	x axis={grid, label={Rules Pruned}},
	y axis={grid, label={Testing Accuracy}},
	leaf={label in legend={text=Testing Accuracy}}]
data [set=leaf]{
x, y
0, 62.40
1, 62.70
2, 62.70
3, 62.80
4, 62.80
5, 63.00
6, 63.00
7, 63.00
8, 63.00
9, 63.10
10, 63.00
11, 63.00
12, 63.20
13, 63.10
14, 63.10
15, 63.10
16, 63.00
17, 62.80
18, 62.70
19, 63.90
20, 64.10
21, 64.10
22, 64.70
23, 64.70
24, 65.90
25, 65.70
26, 65.70
27, 65.90
28, 65.90
29, 65.90
30, 66.80
31, 66.40
32, 67.00
33, 67.00
34, 67.00
35, 66.90
36, 67.10
37, 67.10
38, 67.10
39, 66.90
40, 66.80
41, 66.90
42, 67.00
43, 66.80
44, 66.80
45, 66.80
46, 66.90
47, 67.20
48, 67.20
49, 67.20
50, 67.20
51, 67.20
52, 67.50
53, 67.50
54, 67.50
55, 67.50
56, 67.60
57, 67.70
58, 67.70
59, 67.50
60, 67.60
61, 67.50
62, 67.40
63, 67.30
64, 67.30
65, 67.30
66, 67.30
67, 67.30
68, 67.30
69, 67.30
70, 67.30
71, 67.30
72, 67.30
73, 67.30
74, 67.40
75, 67.70
76, 67.70
77, 67.60
78, 67.60
79, 67.60
80, 67.70
81, 67.60
82, 67.60
83, 67.60
84, 67.50
85, 67.60
86, 67.40
87, 67.40
88, 67.40
89, 67.30
90, 67.30
91, 67.30
92, 67.30
93, 67.10
94, 67.10
95, 67.10
96, 67.10
97, 67.10
98, 67.10
99, 67.20
100, 67.30
101, 67.20
102, 67.50
103, 67.50
104, 67.90
105, 67.90
106, 67.90
107, 67.90
108, 67.90
109, 67.90
110, 68.00
111, 68.00
112, 68.00
113, 68.00
114, 68.00
115, 68.00
116, 68.00
117, 68.00
118, 68.00
119, 68.00
120, 68.00
121, 68.60
122, 68.60
};
\end{tikzpicture}

\section*{Decision Forests (Experiment 5)}
A decision forest was constructed using $5, 10, ... 50$ trees  with $\sqrt{N}$ where $N$ are the attributes occuring in the $1000$ trees and the following results were obtained:

\begin{tikzpicture}[scale=1.5]
\datavisualization [scientific axes=clean,
	x axis={grid, label={Noise Threshold}, ticks={tick unit=\%}},
	y axis={grid, label={Accuracy}, ticks ={tick unit=\%}},
	visualize as smooth line/.list={trainacc, testacc},
	style sheet=vary hue,
	style sheet=cross marks,
	trainacc={label in legend={text=Accuracy on Training Data}},
	testacc={label in legend={text=Accuracy on Testing Data}}]
data [set=trainacc] {
	x, y
	5, 76.68
	10, 84.22
	15, 89.14
	20, 89.74
	25, 93.76
	30, 89.34
	35, 91.75
	40, 91.75
	45, 93.96
	50, 95.07
}
data [set=testacc] {
	x, y
	5, 53.59
	10, 55.01
	15, 59.37
	20, 56.43
	25, 59.27
	30, 56.12
	35, 58.35
	40, 57.04
	45, 59.37
	50, 60.28
};
\end{tikzpicture}
\end{document}